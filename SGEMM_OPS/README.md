# my_SGEMM


## 0.项目基础背景介绍

本项目为一个CUDA项目，通过CUDA编写一个高性能的 SGEMM 算子，前后经过多次优化(体现为不同的kernel版本)。 注意： CUBLAS中的数组都是以列优先的顺序存放的。算子测试部分运用了Benchmark工具(NVIDIA Nsight Systems)。主要用到的优化策略如下:

1. Loop Tiling  令每一个thread处理 32 * 32 大小的子矩阵，将原来的1024单层循环再细分为了 32 * 32 的两层循环结构
2. 采取了"舍弃同一线程之内的访问空间局部性，而追求不同线程对数据访问的并行性" 这一策略，实际上是对最内层 for 循环中对数据的读取进行了顺序上的变换，将线程 Id 与 变化更慢的那个矩阵索引相映射，实现了相邻两个线程所读取的矩阵地址空间(二维矩阵，一维数组存放形式)相隔了32，避免出现数据竞争。通过BenchMark测试发现此策略带来了一倍的性能优化
3. 增加每一个Thread的workloads,最终将大小设置为256,经过测试，此优化带来的性能提升是两倍
4. 采用向量化指令,使用 float4 类型变量进行运算
5. 对向量化运算做进一步处理，将每次SIMD操作对象由 4*1 向量改为 4×4的子矩阵，这一步带来的性能提升是1700GFLOPS
6. 进一步增加每一个ThreadBlock的workloads，将其设置为1024(因为CUDA的基本调度单位是TB，TB的大小对CUDA程序的影响性能也较大)，这一步带来的GFLOPS性能提升是 700 GFLOPS
6. 对最内存循环运用了 Loop unrolling,展开系数为 4




## 优化记录

culabs.sgemm:   方阵为2048*2048

(每次下一个版本的kernel都是基于上一个版本的kernel再做优化的)

| kernel                                                                       | data_size | GFLOPS |
| ---------------------------------------------------------------------------- | --------- | ------ |
| cublas                                                                       | 2048*2048 | 2477   |
| kernel1(naive)                                                               | 2048*2048 | 386    |
| kernel2(naive loop tiling)                                                   | 2048*2048 | 154    |
| kernel3(saving a register)                                                   | 2048*2048 | 154    |
| kernel4(利用了线程空间访问并行性)                                            | 2048*2048 | 279    |
| kernel5(increse the worklloads of per thread)                                | 2048*2048 | 830    |
| kernel6(Vectorized)                                                          | 2048*2048 | 930    |
| kernel7(之前shared memory一次只处理一个4×1*向量，现在一次处理4*×4子矩阵) | 2048*2048 | 2690   |
| kernel8(进一步增加了每个TB的 workloads)                                      | 2048*2048 | 3280   |
|                                                                              |           |        |
|                                                                              |           |        |
|                                                                              |           |        |
|                                                                              |           |        |
|                                                                              |           |        |
|                                                                              |           |        |
